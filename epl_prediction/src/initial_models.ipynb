{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../preprocessed_data/data_with_removals_encoded.csv\")\n",
    "#data = pd.read_csv(\"../preprocessed_data/data_without_promoted_teams.csv\")\n",
    "#data = pd.read_csv(\"../preprocessed_data/data_with_30_most_important_features.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['FTR_encoded'] = label_encoder.fit_transform(data['FTR'])\n",
    "data_encoded =data.drop(columns=['FTR'])\n",
    "\n",
    "data_encoded = data_encoded.astype({col: np.float64 for col in data_encoded.columns if col != 'FTR_encoded'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validation, test split\n",
    "temp_data, test_data = train_test_split(data_encoded, test_size=0.1, random_state=42)\n",
    "train_data, valid_data = train_test_split(temp_data, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(train_data.drop(columns=['FTR_encoded']).values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data['FTR_encoded'].values, dtype=torch.long)\n",
    "X_valid = torch.tensor(valid_data.drop(columns=['FTR_encoded']).values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(valid_data['FTR_encoded'].values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1e-3\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def l1_loss(self):\n",
    "        # Calculate L1 regularization term\n",
    "        l1_reg = torch.tensor(0., requires_grad=True)\n",
    "        for param in self.parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        return l1_reg\n",
    "    def l2_loss(self):\n",
    "        # Calculate L2 regularization term\n",
    "        l2_reg = torch.tensor(0., requires_grad=True)\n",
    "        for param in self.parameters():\n",
    "            l2_reg = l2_reg + torch.norm(param, 2)\n",
    "        return l2_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 32\n",
    "output_size = 3\n",
    "\n",
    "reg = None\n",
    "\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 100\n",
    "\n",
    "model = SimpleMLP(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    return correct / targets.size(0)\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    if reg == 'l1':\n",
    "        l1_loss = model.l1_loss()\n",
    "        loss += lambda_ * l1_loss\n",
    "    elif reg == 'l2':\n",
    "        l2_loss = model.l2_loss()\n",
    "        loss += lambda_ * l2_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(outputs, y_train)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = model(X_valid)\n",
    "        valid_loss = criterion(valid_outputs, y_valid)\n",
    "        valid_accuracy = calculate_accuracy(valid_outputs, y_valid)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "        valid_losses.append(valid_loss.item())\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Train Acc: {train_accuracy:.4f}, Valid Loss: {valid_loss.item():.4f}, Valid Acc: {valid_accuracy:.4f}')\n",
    "\n",
    "# Plot the accuracies\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the losses\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for varying the number of hidden units and layers\n",
    "\n",
    "| Hidden Units | # Hidden Layers | Train Accuracy | Validation Accuracy | Train Loss | Validation Loss |\n",
    "|--------------|----------------|----------------|---------------------|------------|-----------------|\n",
    "| 32           | 1              | 0.5441         | 0.5274              | 0.9547     | 0.9837          |\n",
    "| 32           | 2              | 0.5495         | 0.5323              | 0.9483     | 0.9861          |\n",
    "| 32           | 3              | 0.5448         | 0.5225              | 0.9546     | 0.9872          |\n",
    "| 64           | 1              | 0.5546         | 0.5204              | 0.9396     | 0.9911          |\n",
    "| 64           | 2              | 0.5734         | 0.5225              | 0.9133     | 0.9966          |\n",
    "| 64           | 3              | 0.5797         | 0.5211              | 0.9051     | 1.0065          |\n",
    "| 128          | 1              | 0.5794         | 0.5190              | 0.9042     | 0.9936          |\n",
    "| 128          | 2              | 0.6303         | 0.5211              | 0.8335     | 1.0341          |\n",
    "| 128          | 3              | 0.7127         | 0.4831              | 0.6893     | 1.1963          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the initial results, the best model is the one with 64 hidden units and 2 hidden layers, with a train accuracy of 0.5734 and a validation accuracy of 0.5225.\n",
    "\n",
    "Let's see if we can further improve the model with other regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout results with different inclusion probabilities\n",
    "\n",
    "| Inclusion Probability | Train Accuracy | Validation Accuracy | Train Loss | Validation Loss |\n",
    "|-----------------------|----------------|---------------------|------------|-----------------|\n",
    "| 0.6                   | 0.5355         | 0.5218              | 0.9820     | 0.9822          |\n",
    "| 0.7                   | 0.5355         | 0.5218              | 0.9820     | 0.9822          |\n",
    "| 0.85                  | 0.5407         | 0.5197              | 0.9691     | 0.9860          |\n",
    "| 0.97                  | 0.5450         | 0.5239              | 0.9567     | 0.9865          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a very high inclusion probability has a marginal effect on the validation accuracy, though probably not enough to be statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 regularization results\n",
    "\n",
    "| Lambda | Train Accuracy | Validation Accuracy | Train Loss | Validation Loss |\n",
    "|-----------------------|----------------|---------------------|------------|-----------------|\n",
    "| 5e-4                  | 0.5330         | 0.5147              | 1.0255     | 0.9831          |\n",
    "| 1e-3                  | 0.5341         | 0.5183              | 1.0471     | 0.9839          |\n",
    "| 5e-3                  | 0.4542         | 0.4677              | 1.1763     | 1.0052          |\n",
    "| 5e-2                  | 0.4542         | 0.4677              | 2.1595     | 1.0731          |\n",
    "\n",
    "It seems that l1 regularization is not a good fit for this model... it actually had a negative effect on the validation accuracy and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 regularization results\n",
    "\n",
    "| Lambda | Train Accuracy | Validation Accuracy | Train Loss | Validation Loss |\n",
    "|-----------------------|----------------|---------------------|------------|-----------------|\n",
    "| 5e-4                  | 0.5260         | 0.5126              | 1.1570     | 0.9888          |\n",
    "| 1e-3                  | 0.5423         | 0.5211              | 0.9658     | 0.9889          |\n",
    "| 5e-3                  | 0.5420         | 0.5239              | 0.9998     | 0.9840          |\n",
    "| 5e-2                  | 0.5235         | 0.5140              | 1.1638     | 0.9871          |\n",
    "\n",
    "It seems that l2 regularization with a lambda of 5e-3 is the best fit for this model of the lot, however still performs slightly worse than the model without regularization. Higher lambdas do however help to reduce the overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are two possible issues here:\n",
    "1. The model is too simple and is not able to capture the complexity of the data (albeit increasing complexity decreases the validation accuracy and increases overfitting)\n",
    "2. The data itself is too noisy or contains too much irrelevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps:\n",
    "- Experiment with different feature selections - particularly with less features.\n",
    "- experiment with different model architectures and layers, as well as different combinations of dropout and regularization\n",
    "- Research into different techniques that could help boost model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epl-prediction-fZ7oNFll-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
